# Spectral Holomorphic Fields for Mechanistic Interpretability

Author: Faycal Farhat  
Last modified: August 18, 2025

## Abstract
The internal mechanisms of large language models (LLMs) remain difficult to characterize. This document proposes a mathematical framework that models learned semantic structure as complex-analytic fields on Riemann surfaces. We explore whether linguistic compositionality may align with properties of holomorphic functions: root morphemes corresponding to branch points, affixes to poles and residues, and composition to analytic continuation[^stat-method]. We outline an alternating-projections method to fit a singularity-aware field consistent with attention-derived constraints and token semantics, and we propose validation studies that test falsifiable predictions about morphological composition and metaphor. The approach treats complex analysis as a modeling framework for internal representations rather than claiming that language inherently exhibits holomorphic structure.

## 1.0 Properties, Substrates, and Linguistic Universals

Properties are entities that can be predicated of things or attributed to them, often referred to as predicables, attributes, qualities, features, characteristics, or types. They represent ways things are, exemplified or instantiated by objects. Properties are characterized as exemplifiables, capable of being instantiated by multiple objects, thus "shared" and multi-located.

In the context of language models, this property-theoretic framework reveals some interesting connections to our morphemic field analysis:

### Tokens as Substrates
Discrete tokens serve as the concrete particulars or substrates that bear semantic properties.

### Morphemes as Universal Properties  
Morphemes function as universals—semantic properties capable of multiple instantiation across different token substrates. The property "negation" (exemplified by "un-") can be instantiated across countless contexts while maintaining its essential semantic character.

### Fields as Property Distributions
The holomorphic fields we construct represent continuous distributions of property instantiation patterns. Rather than arbitrary mathematical impositions, these fields aims at capturing how these properties (morphemes) are distributed across substrates (tokens) in semantic space.

### Attention as Property Attribution
Attention mechanisms implement the fundamental operation of property attribution—connecting substrates to their properties through learned patterns. The kernel methods we employ formalize this attribution process mathematically.

This philosophical grounding attempts the transformation of the mathematical treatment from speculative modeling to formal representation of property-theoretic structure inherent in language.

## 1. Core Hypothesis: The Holomorphic Language Model
We propose that LLM semantic representations may be usefully modeled as a complex-analytic structure in which linguistic morphemes map to features of a holomorphic function on a Riemann surface.

- Root morphemes as branch points. The core semantic unit of a word (e.g., “happy”) is represented by a multi-valued function with different senses corresponding to different sheets; the branch point anchors the core concept.
- Affixes as poles and residues. Bound morphemes (prefixes/suffixes) that modify meaning are represented as poles; the residue quantifies the specific semantic transformation.
  - Example: “un-happy-ness”
    - “happy” is the central holomorphic function;
    - “un-” acts as a pole implementing a negation-like transformation;
    - “-ness” is another singularity whose residue effects an adjective→noun transformation.
- Compositionality as analytic continuation. Combining morphemes to form words and sentences is analogous to analytic continuation. Given local behavior at branch points and poles, global behavior (expression meaning) is constrained by complex-analytic structure.

## 2. Methodology
We organize the program into four phases.

### Phase 1: Spectral analysis of attention kernels
Formalize a kernel-ridge perspective on attention and analyze a symmetric resonance operator to identify primary “standing-wave” modes of information flow per head:
- Define influence ("hat") matrices $$H_{qk}(\lambda)$$, $$H_{kk}(\lambda)$$ and the symmetric resonance operator $$S_q(\lambda) = H_{qk}(\lambda)\, H_{kq}(\lambda)$$
- Use eigendecomposition and stability diagnostics (e.g., eigengap) to identify salient modes.

### Phase 2: Semantic field construction
Project discrete token embeddings into a continuous complex plane (2D). Initial work may use PCA; we will evaluate manifold-learning projections (e.g., UMAP) that better preserve semantic topology and may be more compatible with a holomorphic structure.

### Phase 3: Alternating Projections
Construct a field $$\psi$$ that satisfies three constraints to the extent possible:
- $$P_{\text{att}}$$ (attention projection): align field gradients with spectral modes from Phase 1
- $$P_{\text{sem}}$$ (semantic projection): match field values at token locations to embeddings
- $$P_{\text{hol}}$$ (holomorphic projection): satisfy Cauchy–Riemann equations away from a discoverable set of singularities (poles and branch points)

### Phase 4: Validation Studies
Propose running the algorithm on multiple LLMs (e.g., Gemma, Llama series). Test predictions:
- **Singularity detection**: Allow poles at affix locations and assess whether Cauchy–Riemann error decreases materially
- **Compositional prediction**: Combine the fields/singularities for "happy" and "un-" and evaluate whether the predicted field approximates that of "unhappy"[^1]

## 3. Positioning, falsifiability, and scope
This work treats complex analysis as a modeling lens for internal representations, not a claim that language itself is holomorphic. We distinguish genuine structure from clever analogy via three criteria:

- Falsifiable prediction. Does the framework make testable predictions that generalize with statistical significance? For example, characterize an "un-" operator from one transformation (e.g., safe→unsafe) and test whether it transfers to other roots (stable→unstable) with measurable predictive accuracy[^stat-method].
- Explanatory compression. Does the framework yield simpler explanations for behaviors (e.g., "deceptive instrumental alignment") than dense attribution graphs—e.g., as interacting poles/residues—with comparable or better predictive value under rigorous comparison[^stat-method]?
- Effective control. Can we impose boundary conditions or intervene on identified structures to shift behavior in statistically measurable ways (e.g., reduce deceptive outputs) under pre-registered evaluation protocols[^stat-method]?

These criteria move beyond analogy toward empirical evaluation. The initial morphology-focused tests are calibration exercises; generality must be established on broader phenomena.

## 4. Modeling linguistic phenomena
- Polysemy (multiple meanings). Modeled as multiple sheets of a Riemann surface; root morphemes act as branch points. Context selects a sheet along a path in the field. E.g., “bank” in “river bank” versus “investment bank.”
- Context dependence. Fields are constructed from contextual embeddings; the field for “cool” reflects different neighborhoods in “a cool breeze” versus “a cool idea.”
- Metaphor and pragmatics (research frontier). Hypothesis: certain metaphors correspond to geometric transformations (e.g., conformal maps) that stretch/warp source-domain regions toward target-domain regions, preserving local structure that supports cross-domain transfer.

## 5. Computational feasibility and operational design
To address tractability, we separate offline compilation from online attestation.

- Offline compilation (per model version).
  - Construct semantic fields for benchmark concepts.
  - Analyze topology to identify morphemic poles and boundaries.
  - Decompile and document a library of verified circuits (paired with independent attribution analysis).
  - This is a one-time, intensive analysis per major version (analogous to building a map); not performed at inference.

- Online attestation (lightweight check at inference).
  - Compute low-dimensional RKHS “signatures” (e.g., GCV, degrees of freedom, eigengap) on small $d_h \times d_h$ matrices per head.
  - Use cached certificates (plan cache) for frequent, pre-certified paths.
  - Exploit per-head parallelism and GPU-friendly linear algebra.

## 6. Experimental design: metaphor validation
Hypothesis: A metaphor like “my lawyer is a shark” corresponds to a consistent geometric transformation T_shark applied to the “lawyer” region.

- Step A: Characterize T_shark.
  - Obtain fields ψ_baseline (“I spoke with my lawyer”) and ψ_metaphor (“My lawyer is a shark”).
  - Solve for T_shark such that ψ_metaphor ≈ T_shark(ψ_baseline).
- Step B: Test generalization.
  - For a new source domain (“executive”), compute φ_baseline.
  - Predict φ_pred = T_shark(φ_baseline) and compare to φ_actual from “The executive is a shark.”
- Evidence criterion: statistically significant reduction in distance (e.g., L2) between prediction and actual across multiple source domains[^stat-method].
- Boundary conditions and non-compositions.
  - Not all operators should generalize (e.g., “time is money” vs “love is money”); failure maps the operator’s domain.
  - Compositional metaphors (e.g., “bloodthirsty shark”) may correspond to specific operators rather than simple compositions; this is testable by comparing T_bloodthirsty_shark to T_shark ∘ T_bloodthirsty.
- Cross-model comparison.
  - Differences across models (e.g., Claude vs Gemma) would suggest training-dependent geometric structures, enabling comparative analysis.

## 7. Coverage and inference-time policy
We adopt a defense-in-depth, tiered policy.

- Tier 1 (cache hit). Use pre-certified plan cache entries whenever available for common intents.
- Tier 2 (cache miss). Run lightweight RKHS diagnostics in real time for spectral signatures; more expensive than a lookup but far cheaper than full field reconstruction.
- Tier 3 (high-risk fallback). If diagnostics are ambiguous or high-risk, withhold attestation and reroute to safer plans or human-in-the-loop review.

## 8. Scope, cost, and Bayesian concept basis
The approach is targeted, not exhaustive.

- Curated basis. Identify a few hundred welfare-critical concepts (e.g., deception, harm, commitment, power, consent, corrigibility).
- Bayesian generative approach.
  - Define a semantic basis via clustering/set-theoretic selection in embedding space.
  - Build prior fields only for the basis to create a "prior map" of semantic geometry.
  - For new concepts, infer fields as combinations of basis fields rather than rebuilding from scratch.
- Model updates.
  - Major versions likely require full recompilation of basis concepts.
  - Minor versions: run a “geometric diff” on RKHS signatures and recompile only concepts showing drift.

## 9. Preliminary negative results and limitations
A naive attempt to fit a simple holomorphic function globally over the semantic plane produced a negative result, consistent with the hypothesis that affixes induce poles and root polysemy induces branch points. Limitations and cautions:

- Reported statistics and diagnostics are setup-dependent; replication across datasets, tasks, and models is required.
- The morphological focus is a calibration step; broader linguistic phenomena (e.g., pragmatics, metaphor) require separate tests and may yield negative results in some regimes.
- Attestation signals should be evaluated against strong baselines; we avoid claims of guarantees.

## 10. Expected contributions
If supported empirically, this framework could provide:

- A compact language for describing internal computations as continuous fields.
- Practical tools for identifying and validating specific circuits (e.g., negation-like operators).
- New architectural ideas and constraints informed by geometric/analytic structure.

---

[^stat-method]: Complete statistical methodology and validation protocols: [../08_Appendix/08.5_methodology_statistical_significance.md](../08_Appendix/08.5_methodology_statistical_significance.md)

### Appendix A — RKHS attestation primer (per-head, per-intent)

For a head with queries $Q \in \mathbb{R}^{T \times d_h}$, keys $Z \in \mathbb{R}^{T \times d_h}$:
- Cross/Gram kernels: $$K_{qk} = Q Z^T$$, $$K_{kk} = Z Z^T$$
- Influence (hat) matrices with ridge $$\lambda > 0$$:
  - $$H_{qk}(\lambda) = K_{qk} (K_{kk} + \lambda I)^{-1}$$
  - $$H_{kk}(\lambda) = K_{kk} (K_{kk} + \lambda I)^{-1}$$
- Diagnostics (per head, per intent):
  - $$\mathrm{GCV}(\lambda) = \| (I - H_{kk}) V \|_F^2 / (T - \mathrm{tr}\, H_{kk})^2$$
  - $$\mathrm{DoF} = \mathrm{tr}\, H_{kk} = \sum_i \dfrac{\sigma_i^2}{\sigma_i^2 + \lambda} \quad (Z = U \, \Sigma \, W^T)$$
- Symmetric resonance operator $$S = H_{qk} H_{kq}$$ (PSD):
  - Monitor eigengap, spectral entropy; power iteration $$S^m \leftrightarrow$$ multi‑phase resonance.
  - Stability signals: resonance concentration, leverage extremes, $$\kappa(K_{kk} + \lambda I)$$.
- Policy: per-commitment thresholds on {$$\mathrm{GCV}$$, $$\mathrm{DoF}$$, eigengap, resonance concentration}.
  - Router blocks/reroutes if the signature drifts.
- Compute: SVD/Woodbury on $$d_h \times d_h$$ (small), batched per layer/head.

---

[^1]: See Appendix A for preliminary investigation methodology  
[^2]: See Appendix B for statistical framework development  
[^3]: See Appendix C for circuit analysis protocols  
[^4]: See Appendix D for validation methodology
