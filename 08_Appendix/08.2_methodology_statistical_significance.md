# Statistical Methodology for Resonant Attention Analysis

## 1. Notation and Preliminaries

We adopt standard attention notation. For a single layerâ€“head and a given input sequence of length T with head dimension d_h, let queries and keys be matrices:

$$
Q \in \mathbb{R}^{T\times d_h}, \quad Z \in \mathbb{R}^{T\times d_h}.
$$

Define cross- and Gram-kernels:

$$
K_{qk} = Q Z^\top, \qquad K_{kk} = Z Z^\top.
$$

When using RKHS-style influence operators with ridge parameter \(\lambda > 0\):

$$
H_{qk}(\lambda) = K_{qk}\,(K_{kk}+\lambda I)^{-1}, \qquad H_{kk}(\lambda) = K_{kk}\,(K_{kk}+\lambda I)^{-1}.
$$

A symmetric resonance operator may be constructed as:

$$
S(\lambda) = H_{qk}(\lambda)\,H_{kq}(\lambda), \qquad S \succeq 0.
$$

## 2. Concentration Metrics

**Definition 2.1** (Bidirectional Resonance Map). For attention matrices $Q, Z \in \mathbb{R}^{T \times d_h}$, the resonance map is defined as:
$$
R = (Q Z^\top) \odot (Z Q^\top)
$$
where $\odot$ denotes element-wise multiplication.

**Definition 2.2** (Herfindahl-Hirschman Concentration Index). For resonance matrix $R$, let $r_{ij} = \max(R_{ij}, 0)$. The normalized HHI is:
$$
\operatorname{HHI}(R) = \frac{\sum_{i,j} r_{ij}^2}{\left(\sum_{i,j} r_{ij}\right)^2} \cdot T^2
$$

**Definition 2.3** (Entropy-based Concentration). Define $p_{ij} = r_{ij}/\sum_{a,b} r_{ab}$ where $r_{ij} = \max(R_{ij},0)$. The concentration entropy is:
$$
\operatorname{CE}(R) = -\sum_{i,j} p_{ij} \log p_{ij}
$$

## 3. Hypothesis Testing Framework

**Definition 3.1** (Null Models). Let $\mathcal{H}_0^{(1)}$ denote the token permutation null where input tokens are permuted within segments while preserving local structure. Let $\mathcal{H}_0^{(2)}$ denote the head-sequence bootstrap null obtained by resampling across sequences and mixing heads within layer groups.

**Definition 3.2** (Test Statistic). For concentration measure $C$ and null distribution samples $\{C_b\}_{b=1}^B$ with empirical mean $\hat\mu$ and standard deviation $\hat\sigma$, the standardized test statistic is:
$$
Z = \frac{C_{\text{obs}} - \hat\mu}{\hat\sigma}
$$

**Definition 3.3** (P-value Computation). Under normality assumption, the two-sided p-value is $p = 2\Phi(-|Z|)$. For non-parametric testing, the empirical p-value is:
$$
\hat{p} = \frac{1}{B}\sum_{b=1}^B \mathbf{1}\{C_b \geq C_{\text{obs}}\}
$$

**Null Hypothesis**: $H_0: C_{\text{obs}} \sim \mathcal{H}_0$

**Alternative Hypothesis**: $H_1: C_{\text{obs}} > \mathbb{E}_{\mathcal{H}_0}[C]$

## 4. Multiple Comparison Corrections

**Definition 4.1** (Bonferroni-Holm Procedure). For ordered p-values $p_1 \leq p_2 \leq \cdots \leq p_m$, reject $H_{0i}$ if $p_i \leq \alpha/(m-i+1)$ for all $j \leq i$.

**Definition 4.2** (Benjamini-Hochberg FDR Control). For target FDR level $q$, reject $H_{0i}$ for $i \leq k$ where:
$$
k = \max\left\{i: p_i \leq \frac{i \cdot q}{m}\right\}
$$

**Definition 4.3** (Family-Wise Error Rate). The probability of making one or more Type I errors:
$$
\text{FWER} = P\left(\bigcup_{i \in H_0} \{p_i \leq \alpha\}\right)
$$

## 5. RKHS Spectral Diagnostics

**Definition 5.1** (Generalized Cross-Validation). For ridge parameter $\lambda$ and validation matrix $V$:
$$
\operatorname{GCV}(\lambda) = \frac{\| (I - H_{kk}(\lambda))\,V \|_F^2}{\big(T - \operatorname{tr}\,H_{kk}(\lambda)\big)^2}
$$

**Definition 5.2** (Effective Degrees of Freedom). For eigenvalues $\{\sigma_i^2\}$ of $K_{kk}$:
$$
\operatorname{DoF}(\lambda) = \operatorname{tr}\,H_{kk}(\lambda) = \sum_i \frac{\sigma_i^2}{\sigma_i^2 + \lambda}
$$

**Definition 5.3** (Spectral Gap). For eigenvalues $\sigma_1^2 \geq \sigma_2^2 \geq \cdots$ of the symmetric operator $S(\lambda)$:
$$
\text{Gap}_k = \sigma_k^2 - \sigma_{k+1}^2
$$

**Definition 5.4** (Spectral Entropy). For normalized eigenvalues $\tilde{\sigma}_i^2 = \sigma_i^2/\sum_j \sigma_j^2$:
$$
H_{\text{spec}} = -\sum_i \tilde{\sigma}_i^2 \log \tilde{\sigma}_i^2
$$

## References
- Wasserman, L. (2004). *All of Statistics: A Concise Course in Statistical Inference*. Springer-Verlag.
- Goulet Coulombe (2025), Ordinary Least Squares as an Attention Mechanism.
- Wahba & Wang (2015), An Overview of RKHS Methods.