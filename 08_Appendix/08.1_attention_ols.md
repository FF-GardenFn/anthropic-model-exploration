# The OLS-Attention Equivalence: Mathematical Foundations and RKHS Extensions

## 1. Mathematical Preliminaries and Notation

Consider a regression problem with training data $(X_{\text{train}}, \mathbf{y}) \in \mathbb{R}^{n \times p} \times \mathbb{R}^n$ and test points $X_{\text{test}} \in \mathbb{R}^{m \times p}$. The standard OLS predictor is given by:

$$\hat{\mathbf{y}}_{\text{test}} = X_{\text{test}}(X_{\text{train}}^T X_{\text{train}})^{-1} X_{\text{train}}^T \mathbf{y}$$

Define the notation for attention mechanisms with sequence length $T$ and head dimension $d_h$:
- $Q \in \mathbb{R}^{T \times d_h}$ = query matrix
- $K \in \mathbb{R}^{T \times d_h}$ = key matrix  
- $V \in \mathbb{R}^{T \times d_h_v}$ = value matrix
- $K_{qk} = QK^T$ = cross-kernel matrix
- $K_{kk} = KK^T$ = Gram kernel matrix

The regularized formulations require:
- $H_{qk}(\lambda) = K_{qk}(K_{kk}+\lambda I)^{-1}$ = cross hat matrix
- $S = H_{qk}H_{qk}^T$ = symmetric resonance operator
- $\lambda \geq 0$ = regularization parameter

## 2. The Coulombe Reformulation

Coulombe's central insight transforms OLS from matrix inversion to similarity-based prediction through orthonormal embeddings. The reformulation proceeds via eigendecomposition of the Gram matrix.

### 2.1 Eigendecomposition and Embedding Construction

Let $X_{\text{train}}^T X_{\text{train}} = U\Lambda U^T$ where $U \in \mathbb{R}^{p \times p}$ contains orthonormal eigenvectors and $\Lambda = \text{diag}(\lambda_1, \ldots, \lambda_p)$ with eigenvalues $\lambda_i \geq 0$.

Define the orthonormal embeddings:
$$F_{\text{train}} = X_{\text{train}} U \Lambda^{-1/2}$$
$$F_{\text{test}} = X_{\text{test}} U \Lambda^{-1/2}$$

These satisfy $F_{\text{train}}^T F_{\text{train}} = I_p$, establishing orthonormality.

### 2.2 The Similarity-Based Formulation

Substituting the eigendecomposition into the OLS formula yields:
$$(X_{\text{train}}^T X_{\text{train}})^{-1} = U\Lambda^{-1}U^T$$

Therefore:
$$\hat{\mathbf{y}}_{\text{test}} = X_{\text{test}} U \Lambda^{-1/2} \Lambda^{-1/2} U^T X_{\text{train}}^T \mathbf{y}$$

Using the embedding definitions:
$$\hat{\mathbf{y}}_{\text{test}} = F_{\text{test}} F_{\text{train}}^T \mathbf{y}$$

This reformulation expresses OLS predictions as inner products between orthonormal embeddings of test and training data.

### 2.3 Mathematical Equivalence to Attention

The similarity matrix $F_{\text{test}} F_{\text{train}}^T \in \mathbb{R}^{m \times n}$ computes inner products between embedded test queries and training keys. Element $(i,j)$ represents the similarity between test point $i$ and training point $j$.

The OLS prediction for test point $i$ becomes:
$$\hat{y}_i = \sum_{j=1}^n (F_{\text{test}} F_{\text{train}}^T)_{ij} y_j$$

This precisely matches the attention computation:
$$\text{Output}_i = \sum_{j=1}^n A_{ij} V_j$$

where $A_{ij}$ are attention weights and $V_j$ are value vectors.

## 3. Regularization and the Ridge Connection

### 3.1 Ridge Regression Formulation

The regularized version replaces $(X_{\text{train}}^T X_{\text{train}})^{-1}$ with $(X_{\text{train}}^T X_{\text{train}} + \lambda I)^{-1}$:

$$\hat{\mathbf{y}}_{\text{test}}^{(\lambda)} = X_{\text{test}}(X_{\text{train}}^T X_{\text{train}} + \lambda I)^{-1} X_{\text{train}}^T \mathbf{y}$$

Let $X_{\text{train}}^T X_{\text{train}} + \lambda I = U(\Lambda + \lambda I)U^T$. The regularized embeddings are:
$$F_{\text{train}}^{(\lambda)} = X_{\text{train}} U (\Lambda + \lambda I)^{-1/2}$$
$$F_{\text{test}}^{(\lambda)} = X_{\text{test}} U (\Lambda + \lambda I)^{-1/2}$$

### 3.2 Attention as Implicit Ridge Regression

The regularized prediction becomes:
$$\hat{\mathbf{y}}_{\text{test}}^{(\lambda)} = F_{\text{test}}^{(\lambda)} (F_{\text{train}}^{(\lambda)})^T \mathbf{y}$$

The regularization parameter $\lambda$ controls the eigenvalue shrinkage: 
$$\frac{\lambda_i}{\lambda_i + \lambda} \mapsto \text{shrinkage factor for eigenvalue } \lambda_i$$

Small eigenvalues are heavily regularized while large eigenvalues are preserved, matching attention's implicit regularization through training dynamics.

## 4. RKHS Framework and Extensions

### 4.1 Kernel Representation

Consider the reproducing kernel Hilbert space $\mathcal{H}$ with reproducing kernel $K(x,z)$. The RKHS optimization problem:
$$\min_{f \in \mathcal{H}} \left\{ \frac{1}{n}\sum_{i=1}^n (y_i - f(x_i))^2 + \lambda \|f\|_{\mathcal{H}}^2 \right\}$$

has solution given by the representer theorem:
$$f_\lambda(x) = \sum_{i=1}^n c_i K(x_i, x)$$

where $\mathbf{c} = (K + n\lambda I)^{-1} \mathbf{y}$ and $K_{ij} = K(x_i, x_j)$.

### 4.2 Connection to Attention Kernels

The kernel matrix $K$ in RKHS corresponds to $QK^T$ in attention. The RKHS prediction:
$$f_\lambda(x_{\text{test}}) = \mathbf{k}_{\text{test}}^T (K + n\lambda I)^{-1} \mathbf{y}$$

where $\mathbf{k}_{\text{test}} = (K(x_1, x_{\text{test}}), \ldots, K(x_n, x_{\text{test}}))^T$.

This matches the attention computation with $K_{qk}(K_{kk} + \lambda I)^{-1}$ playing the role of $(K + n\lambda I)^{-1}$.

### 4.3 Spectral Analysis and Degrees of Freedom

The effective degrees of freedom in RKHS are:
$$\text{df}(\lambda) = \text{tr}(K(K + n\lambda I)^{-1}) = \sum_{i=1}^n \frac{\mu_i}{\mu_i + n\lambda}$$

where $\mu_i$ are eigenvalues of $K$. This quantity measures model complexity and appears identically in attention analysis through $\text{tr}(H_{qk})$.

## 5. Generalized Cross-Validation and Model Selection

### 5.1 GCV Criterion

The generalized cross-validation criterion:
$$\text{GCV}(\lambda) = \frac{n^{-1}\|\mathbf{y} - H(\lambda)\mathbf{y}\|^2}{(n^{-1}\text{tr}(I - H(\lambda)))^2}$$

provides automatic regularization parameter selection, where $H(\lambda) = K(K + n\lambda I)^{-1}$.

### 5.2 Application to Attention Analysis

In attention mechanisms, the hat matrix $H_{qk}(\lambda)$ serves the same role as $H(\lambda)$ in RKHS. The spectral properties of $H_{qk}$ determine:
- Model complexity through $\text{tr}(H_{qk})$
- Stability through condition number analysis
- Optimal regularization through GCV-like criteria

## 6. Multi-Head Attention and Ensemble Methods

### 6.1 Ensemble Formulation

Multi-head attention with $h$ heads computes:
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h) W^O$$

Each head operates with projected queries, keys, and values:
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

### 6.2 Statistical Ensemble Interpretation

This corresponds to an ensemble of $h$ ridge regression models, each operating in a different subspace determined by the projection matrices $W_i^Q, W_i^K$. The final linear combination through $W^O$ implements weighted ensemble averaging.

The ensemble reduces variance while potentially increasing bias, matching classical bias-variance decompositions in statistical learning theory.

## 7. Repository Applications and Connections

### 7.1 Circuit Analysis Framework

The spectral decomposition of attention matrices enables circuit analysis through eigenvalue importance ranking. Large eigenvalues correspond to dominant computational pathways, while small eigenvalues represent less critical circuits.

Circuit importance is quantified by:
$$\text{Importance}_i = \frac{\mu_i}{\sum_{j=1}^n \mu_j} \cdot \frac{\mu_i}{\mu_i + \lambda}$$

This metric appears in routed circuit tomography (Section 04.1) for head ranking and routing decisions.

### 7.2 Stability and Monitoring

The condition number $\kappa(K_{kk} + \lambda I) = \frac{\mu_{\max} + \lambda}{\mu_{\min} + \lambda}$ provides stability diagnostics. Large condition numbers indicate potential numerical instability or overfitting.

Dynamic monitoring of eigenvalue distributions enables early detection of model degradation, as implemented in the emergent welfare framework (Section 04.3).

### 7.3 Proof-Carrying Commitments

The spectral properties of $S = H_{qk}H_{qk}^T$ provide attestation signals for model behavior. The eigenvalue spectrum of $S$ encodes information about attention pattern stability and can serve as cryptographic commitments to model state.

## 8. Convergence Analysis and Theoretical Guarantees

### 8.1 Approximation Theory

For functions $f \in \mathcal{H}$ with smoothness $s$, the RKHS approximation satisfies:
$$\|f - f_\lambda\|_{L^2}^2 = O(n^{-2s/(2s+d)})$$

where $d$ is the input dimension. The optimal regularization parameter scales as $\lambda^* = O(n^{-2s/(2s+d)})$.

### 8.2 Attention Convergence

Through the OLS-attention equivalence, transformer attention inherits these approximation guarantees. The learned query-key transformations approximate the optimal RKHS embeddings, with convergence rates depending on the underlying function class.

### 8.3 Finite Sample Analysis

The finite sample risk bound:
$$\mathbb{E}[\|f_\lambda - f^*\|^2] \leq \text{Bias}^2(\lambda) + \text{Variance}(\lambda) + \text{Noise}$$

applies directly to attention mechanisms through the equivalence, providing principled analysis of transformer generalization behavior.

The bias-variance tradeoff is controlled by the eigenvalue shrinkage profile $\frac{\mu_i}{\mu_i + n\lambda}$, which determines the effective model complexity and generalization performance.

---

## References

[^ols-pdf]: See 03_Research: [2504.09663v1.pdf](../03_Research/2504.09663v1.pdf) — Goulet Coulombe (2025), "Ordinary Least Squares as an Attention Mechanism."
[^wahba]: See 03_Research: [wahba.wang.overview2015.pdf](../03_Research/wahba.wang.overview2015.pdf) — Wahba & Wang (2015), "An Overview of RKHS Methods."
[^common]: See Common Foundations: [04_Research_Projects/04.0.5_Common_Foundations/common_foundation.md](../04_Research_Projects/04.0.5_Common_Foundations/common_foundation.md).
[^stat-method]: See Appendix: [Methodology for Statistical Significance and Validation](./08.5_methodology_statistical_significance.md) for definitions, null models, and caveats.
